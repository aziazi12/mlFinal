{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for all models\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataloader import CustomDataloader, CustomImageDataloader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from networks import FullyConnectedNetwork, CNN\n",
    "\n",
    "# Load the processed data from the CSV file\n",
    "data_dict = torch.load('data/ProcessedData.pt')\n",
    "file_names_dict = torch.load('data/ProcessedDataFileNames.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is the code for the first model. Using the CSV files generated from the dataset_processed notebook, we train our model. A training curve graph is generated once the model is complete and saved within the directory to then later be displayed in the analysis notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: 6.3121137619018555\n",
      "Random Baseline MSE: 2134.2127702406337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data loaders for training, validation and testing\n",
    "train_dataloader = CustomDataloader(data_dict['x_train'],data_dict['y_train'], batch_size = 32, randomize=True)\n",
    "val_dataloader = CustomDataloader(data_dict['x_val'],data_dict['y_val'], batch_size = 64)\n",
    "test_dataloader = CustomDataloader(data_dict['x_test'],data_dict['y_test'], batch_size = 64)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "x_train = data_dict['x_train'].numpy()\n",
    "y_train = data_dict['y_train'].numpy()\n",
    "x_val = data_dict['x_val'].numpy()\n",
    "y_val = data_dict['y_val'].numpy()\n",
    "x_test = data_dict['x_test'].numpy()\n",
    "y_test = data_dict['y_test'].numpy()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "predictions = []\n",
    "x_values = []\n",
    "indices = []\n",
    "\n",
    "# Define a range of training set sizes for learning curve\n",
    "training_set_sizes = [int(len(x_train) * fraction) for fraction in np.linspace(0.1, 1.0, 10)]\n",
    "\n",
    "\n",
    "for size in training_set_sizes:\n",
    "    # Fit the model on a subset of the training data\n",
    "    model.fit(x_train[:size], y_train[:size])\n",
    "    \n",
    "    # Predict on the entire training set\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    \n",
    "    # Calculate training loss and append to train_losses\n",
    "    train_loss = mean_squared_error(y_train[:len(y_train_pred)], y_train_pred)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_val_pred = model.predict(x_val)\n",
    "    \n",
    "    # Calculate validation loss and append to val_losses\n",
    "    val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Store predictions, x values, and indices\n",
    "    predictions.extend(y_train_pred)\n",
    "    x_values.extend(x_train[:len(y_train_pred)])\n",
    "    indices.extend(range(len(y_train_pred)))\n",
    "\n",
    "# Calculate final validation loss\n",
    "final_val_loss = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"Final validation loss: {final_val_loss}\")\n",
    "\n",
    "random_baseline = np.random.uniform(min(y_train), max(y_train), size=len(y_train))\n",
    "# Calculate mean squared error (MSE) for the random baseline\n",
    "random_baseline_mse = mean_squared_error(y_train, random_baseline)\n",
    "\n",
    "print(f\"Random Baseline MSE: {random_baseline_mse}\")\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = {'Index': indices, 'X_Value': x_values, 'Prediction': predictions}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "# Create a plot of the learning curve and save it for later use\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(training_set_sizes, train_losses, label='Training loss')\n",
    "plt.plot(training_set_sizes, val_losses, label='Validation loss')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('learning_curve_model_1.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for model 2. Currently it is mostly commented out due to it not being able to actually train the model. The code is commented out in order to provide my thought process and at which point did I have to stop at while working on this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image directory and file names\n",
    "images_directory = 'data/images'\n",
    "x_test_file_names = 'x_test_file_names.csv'\n",
    "x_train_file_names = 'x_train_file_names.csv'\n",
    "x_val_file_names = 'x_val_file_names.csv'\n",
    "\n",
    "# Dataloaders for training, validation and testing\n",
    "test_dataloader = CustomImageDataloader(x_test_file_names, images_directory, batch_size = 64)\n",
    "train_dataloader = CustomImageDataloader(x_train_file_names, images_directory, batch_size = 32)\n",
    "val_dataloader = CustomImageDataloader(x_val_file_names, images_directory, batch_size = 64)\n",
    "\n",
    "# Define the neural network\n",
    "model = CNN(10)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Your training and validation loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     train_epoch_losses = []\n",
    "#     for _ in range(train_dataloader.num_batches_per_epoch):\n",
    "#         train_batch = train_dataloader.fetch_batch()\n",
    "        \n",
    "#         # Transpose the input data to match the model's expected shape [batch_size, 3, 128, 128]\n",
    "#         train_batch['images'] = train_batch['images'].permute(0, 3, 1, 2).float()\n",
    "        \n",
    "#         optimizer.zero_grad()  # Zero the gradients\n",
    "#         output = model(train_batch['images'])  # Forward pass\n",
    "#         loss = criterion(output, train_batch['batch_idx'])  # Compute the loss\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Optimize\n",
    "#         train_epoch_losses.append(loss.item())\n",
    "#     train_losses.append(np.mean(train_epoch_losses))\n",
    "\n",
    "#     val_epoch_losses = []\n",
    "#     for _ in range(val_dataloader.num_batches_per_epoch):\n",
    "#         val_batch = val_dataloader.fetch_batch()\n",
    "        \n",
    "#         # Transpose the validation input data to match the model's expected shape [batch_size, 3, 128, 128]\n",
    "#         val_batch['images'] = val_batch['images'].permute(0, 3, 1, 2).float()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output = model(val_batch['images'])  # Forward pass\n",
    "#             val_loss = criterion(output, val_batch['batch_idx'])  # Compute the loss\n",
    "#             val_epoch_losses.append(val_loss.item())\n",
    "#     val_losses.append(np.mean(val_epoch_losses))\n",
    "\n",
    "\n",
    "# Create a plot of the learning curve and save it for later use\n",
    "# plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "# plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend()\n",
    "# plt.savefig('learning_curve_model_2.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
